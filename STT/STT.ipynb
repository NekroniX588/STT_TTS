{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6cf1c44-1107-40a4-98c9-f58f52575624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "from PIL import Image\n",
    "import Levenshtein as Lev\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca3ee9-8828-4ba0-991b-249f0dd9f913",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb64f70-d5e4-4738-9b05-5adf4ad7e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio(waveform, sample_rate):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    if num_channels == 1:\n",
    "        display(Audio(waveform[0], rate=sample_rate))\n",
    "    elif num_channels == 2:\n",
    "        display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
    "    else:\n",
    "        raise ValueError(\"Waveform with more than 2 channels are not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69aebf1a-f636-4bd5-8964-6fa2b371e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchaudio.datasets.SPEECHCOMMANDS('./SpeechCommands', download = True, subset = \"training\")\n",
    "val_data = torchaudio.datasets.SPEECHCOMMANDS('./SpeechCommands', download = True, subset = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf55c2c-ba73-4d9f-8e5c-32ce41bf6abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommandsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.feature = MelSpectrogram(n_mels = 64)\n",
    "        self.tokenizer = {chr(ord(\"a\") + i): i + 3 for i in range(26)}\n",
    "        self.tokenizer['PAD'] = 0\n",
    "        self.tokenizer['SOS'] = 1\n",
    "        self.tokenizer['EOS'] = 2\n",
    "        \n",
    "        self.inv_tokenizer = {value: key for key, value in self.tokenizer.items()}\n",
    "        \n",
    "    def __encode(self, word):\n",
    "        result = [1]\n",
    "        for w in word:\n",
    "            result.append(self.tokenizer[w])\n",
    "        result.append(2)\n",
    "        return torch.tensor(result)\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        word = []\n",
    "        for t in tokens.tolist():\n",
    "            word.append(self.inv_tokenizer[t])\n",
    "        return \"\".join(word)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        waveform, sample_rate, target, _, _ = self.data[index]\n",
    "        \n",
    "        if sample_rate != 16000:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
    "        features = self.feature(waveform)\n",
    "        \n",
    "        return features[0], self.__encode(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5674afb9-f08c-4a74-b4a6-5fe14eec5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CommandsDataset(train_data)\n",
    "val_dataset = CommandsDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fac5bdb-2f99-481e-bce1-a431036fd6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4652068-316d-40ca-9c7d-b7bfc9996c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    audio = []\n",
    "    audio_len = []\n",
    "    text = []\n",
    "    for sample in batch:\n",
    "        audio.append(sample[0].transpose(0, 1))\n",
    "        audio_len.append(audio[-1].size(0))\n",
    "        text.append(sample[1])\n",
    "    audio = torch.nn.utils.rnn.pad_sequence(audio, batch_first = True)\n",
    "    audio_mask = torch.ones((audio.size(0), audio.size(1)))\n",
    "    for k, length in enumerate(audio_len):\n",
    "        audio_mask[k, :length] = 0\n",
    "    \n",
    "    text = torch.nn.utils.rnn.pad_sequence(text, batch_first = True)\n",
    "    text_mask = torch.ones(text.size())\n",
    "    text_mask[text>0] = 0\n",
    "    \n",
    "    return audio, audio_mask, text, text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58f9e3de-78d9-4098-8ee4-2307e535e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 16, shuffle = True, collate_fn = collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 1, shuffle = False, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a28118f8-fff6-45de-92bb-ee565c06ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16746d3c-66c4-4563-b7e4-e2f3063e742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, audio_mask, text, text_mask = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac2c552f-02f9-438b-8080-4d1ee0c0e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 81, 64])\n",
      "torch.Size([16, 81])\n",
      "torch.Size([16, 7])\n",
      "torch.Size([16, 7])\n"
     ]
    }
   ],
   "source": [
    "print(audio.size())\n",
    "print(audio_mask.size())\n",
    "print(text.size())\n",
    "print(text_mask.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991b2a0-9fca-4471-98b2-47e1c033ddfe",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8425495-8052-4887-aca6-5417b8d27120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c01385c1-edaf-4065-b44e-2ba5fe47ffe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class STT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STT, self).__init__()\n",
    "        \n",
    "        self.prepare = torch.nn.Sequential(\n",
    "                torch.nn.Linear(64, 128),\n",
    "                torch.nn.LayerNorm(128),\n",
    "                torch.nn.Dropout(0.1),\n",
    "                torch.nn.ReLU()\n",
    "            )\n",
    "        self.embeddings = nn.Embedding(29, 128)\n",
    "        self.pos_encoder_src = PositionalEncoding(128, 0.1)\n",
    "        self.pos_encoder_trg = PositionalEncoding(128, 0.1)\n",
    "        self.transformer = torch.nn.Transformer(d_model=128, nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=512, dropout=0.1, batch_first=True)\n",
    "        \n",
    "        self.last_block = nn.Linear(128, 29)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones((sz, sz))) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).bool()\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, audio, audio_padding_mask, text, text_padding_mask):\n",
    "        text_mask_nopeak = self.generate_square_subsequent_mask(text.size(1)).cuda()\n",
    "        \n",
    "        audio = self.prepare(audio)\n",
    "        text = self.embeddings(text)\n",
    "        \n",
    "        audio = self.pos_encoder_src(audio)\n",
    "        text = self.pos_encoder_trg(text)\n",
    "        \n",
    "        out = self.transformer(src = audio, tgt = text, tgt_mask = text_mask_nopeak,\n",
    "                               src_key_padding_mask=audio_padding_mask, tgt_key_padding_mask=text_padding_mask)\n",
    "        \n",
    "        out = self.last_block(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        audio = self.prepare(src)\n",
    "        audio = self.pos_encoder_src(audio)\n",
    "        return self.transformer.encoder(audio)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        text = self.embeddings(tgt)\n",
    "        text = self.pos_encoder_trg(text)\n",
    "        \n",
    "        return self.transformer.decoder(text, memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d1b85e18-6d7b-4df7-afcf-d01f9395cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = STT().cuda()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "optimazer = torch.optim.Adam(model.parameters(), lr = 0.001, betas = (0.9, 0.99))\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimazer, start_factor = 1, end_factor = 0.01, total_iters=10)\n",
    "writer = SummaryWriter()\n",
    "global_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4c2b8cd4-f4c6-4de8-b7f0-5a4610f38f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(audio, audio_mask, text, text_mask).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eb0fd19f-20ef-475d-a3ca-39c6962d49af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# audio = audio.cuda()\n",
    "# audio_mask = audio_mask.cuda()\n",
    "# text = text.cuda()\n",
    "# text_mask = text_mask.cuda()\n",
    "\n",
    "# for i in range(100):\n",
    "#     optimazer.zero_grad()\n",
    "    \n",
    "#     out = model(audio, audio_mask, text[:,:-1], text_mask[:,:-1])\n",
    "# #     print(out.size())\n",
    "# #     print(text.size())\n",
    "#     loss = criterion(out.transpose(1,2), text[:,1:])\n",
    "#     loss.backward()\n",
    "#     optimazer.step()\n",
    "#     print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0e359eb7-32ea-40de-b9a4-bc19eda206e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tokenizer[\"t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f735fefb-c4e1-4364-ad68-86a77ccf25d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOSthreeEOS\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.decode(text[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6d7c31c7-1b1f-4516-9bd8-9defd0863261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# memory = model.encode(audio[1:2], audio_mask)\n",
    "# ys = torch.ones(1, 1).fill_(1).type(torch.long).cuda()\n",
    "# while True:\n",
    "#     print(ys.size())\n",
    "#     tgt_mask = model.generate_square_subsequent_mask(ys.size(1)).cuda()\n",
    "#     out = model.decode(ys, memory, tgt_mask)\n",
    "#     prob = model.last_block(out[:, -1])\n",
    "#     _, next_word = torch.max(prob, dim=1)\n",
    "#     next_word = next_word.item()\n",
    "#     ys = torch.cat([ys.cpu(),torch.ones(1, 1).type(torch.long).fill_(next_word)], dim=1).type(torch.long).cuda()\n",
    "#     if next_word == 2:\n",
    "#         break\n",
    "# print(train_dataset.decode(ys[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fd7e0a1f-d0bf-426a-b80d-6ae069a42003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimazer):\n",
    "    global global_step\n",
    "    model.train()\n",
    "    for audio, audio_mask, text, text_mask in tqdm(loader):\n",
    "        \n",
    "        audio = audio.cuda()\n",
    "        audio_mask = audio_mask.cuda()\n",
    "        text = text.cuda()\n",
    "        text_mask = text_mask.cuda()\n",
    "        \n",
    "        optimazer.zero_grad()\n",
    "        \n",
    "        out = model(audio, audio_mask, text[:,:-1], text_mask[:,:-1])\n",
    "        loss = criterion(out.transpose(1,2), text[:,1:])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimazer.step()\n",
    "        \n",
    "        writer.add_scalar('train/Loss', loss.item(), global_step)\n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b38374ea-357e-4802-9dd7-a957a58e7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, audio, audio_mask):\n",
    "    step = 1\n",
    "    memory = model.encode(audio, audio_mask)\n",
    "    ys = torch.ones(1, 1).fill_(1).type(torch.long).cuda()\n",
    "    while step < 15:\n",
    "#         print(ys.size())\n",
    "        tgt_mask = model.generate_square_subsequent_mask(ys.size(1)).cuda()\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        prob = model.last_block(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys.cpu(),torch.ones(1, 1).type(torch.long).fill_(next_word)], dim=1).type(torch.long).cuda()\n",
    "        step += 1\n",
    "        if next_word == 2:\n",
    "            break\n",
    "    return train_dataset.decode(ys[0])\n",
    "\n",
    "def val(model, loader, criterion):\n",
    "    global global_step\n",
    "    model.eval()\n",
    "    acc = []\n",
    "    for audio, audio_mask, text, text_mask in tqdm(loader):\n",
    "        \n",
    "        audio = audio.cuda()\n",
    "        audio_mask = audio_mask.cuda()\n",
    "        text = text.cuda()\n",
    "        text_mask = text_mask.cuda()\n",
    "        result = predict(model, audio, audio_mask)\n",
    "        target = train_dataset.decode(text[0]) \n",
    "#         print(result, target)\n",
    "        acc.append(Lev.distance(target, result)/len(target))\n",
    "    writer.add_scalar('val/cer', sum(acc)/len(acc), global_step)\n",
    "    print(\"Mean CER:\",sum(acc)/len(acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5c03c35d-969a-42cf-8902-2caf3132e5fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train(model, train_loader, criterion, optimazer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "98f4069b-01c5-4799-924f-151a649abe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8295b13f-0d06-445f-9dea-9a6632d24519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# val(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f339a5-64ea-42a0-a3ed-387337b8e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5303/5303 [01:57<00:00, 45.32it/s]\n",
      "100%|██████████| 9981/9981 [01:43<00:00, 96.63it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CER: 0.18468258383346955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5303/5303 [01:56<00:00, 45.63it/s]\n",
      "100%|██████████| 9981/9981 [01:46<00:00, 93.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CER: 0.14497215066045796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5303/5303 [01:56<00:00, 45.49it/s]\n",
      "100%|██████████| 9981/9981 [01:45<00:00, 94.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CER: 0.11635179268182068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5303/5303 [01:58<00:00, 44.81it/s]\n",
      "100%|██████████| 9981/9981 [01:45<00:00, 94.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CER: 0.1058312455892851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5303/5303 [01:56<00:00, 45.66it/s]\n",
      "100%|██████████| 9981/9981 [01:45<00:00, 94.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CER: 0.0839786153131509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5303/5303 [01:55<00:00, 45.74it/s]\n",
      " 83%|████████▎ | 8300/9981 [01:28<00:15, 105.38it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train(model, train_loader, criterion, optimazer)\n",
    "    val(model, val_loader, criterion)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6b455dc2-e031-4fc5-b9bb-9a5f612953e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ErrorAnalys(model, loader, criterion):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    global global_step\n",
    "    model.eval()\n",
    "    acc = []\n",
    "    for audio, audio_mask, text, text_mask in tqdm(loader):\n",
    "        \n",
    "        audio = audio.cuda()\n",
    "        audio_mask = audio_mask.cuda()\n",
    "        text = text.cuda()\n",
    "        text_mask = text_mask.cuda()\n",
    "        result = predict(model, audio, audio_mask)\n",
    "        target = train_dataset.decode(text[0]) \n",
    "        if result == target:\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "        acc.append(Lev.distance(target, result)/len(target))\n",
    "    print(\"Mean CER:\",sum(acc)/len(acc))\n",
    "    print(\"Pos\", pos/(pos+neg))\n",
    "    print(\"Neg\", neg/(pos+neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ba6928d9-6a13-4832-8123-fa651e1043fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9981/9981 [01:46<00:00, 93.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CER: 0.05821697894886914\n",
      "Pos 0.8478108405971345\n",
      "Neg 0.15218915940286545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ErrorAnalys(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3dbfd-2934-481c-bd43-52f17ee4419a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
